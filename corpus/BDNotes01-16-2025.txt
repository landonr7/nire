BDNotes 01-16-2025: 
USE KARPINSKI CLUSTER!!!

- Cluster Architecture
    - Rack contains 16-64 nodes (or computers)
    - Connected via a switch
    - Each computer contains CPU, memory, and disk

- How can computing power of cluster be utilized when executing a given 
    program?
    - Bring computation close to data
    - Design abstract distributed computation model
    - Store files multiple times for reliability, also distribute computation
    - Map-reduce addresses these issues (programming model)

- Copying data across a network takes time (even with ethernet)
    - Could be the bottleneck of distributed programming

- How can we prevent machine failure?
    - 1 server may last up to 3 years
    - If you have 1,000 servers, expect to lose 1 machine/day

- Storage Infrastructure

- Distributed File System
    - Chunk servers
        - File is split into 16-64 MB chunks
        - Each chunk is replicated
        - Keep reps in different racks
        - Chunke servers also serve as compute servers
    - Master node
        - Stores metadata about where files are stored
    - Client library for file access
        - Talks to master to find chunk servers

- Programming Model: MapReduce
    - Warm-up task ("Hello, World!" of mapreduce):
        - Huge text document
        - Count number of times each distinct word appears in the file
        - Use hashtable to store <word, count> pairs
    - Text document too large for memory, but hashtable of <word, count> pairs fit into memory
        - Just divide file across nodes, but keep adding to hashtable in memory
    - Now, not even hashtable does not fit in memory
        - Captures essence of MapReduce
    - Map: extract data into <key, value> pairs
    - Group By Key: sort and shuffle
    - Reduce: aggregate, summarize, filter or transform <key, <value>* >
        - Keys of equal key are reduced to one key with respective value appended to value vector
    - MapReduce in Parallel
        - Spme N nodes work on mapping
        - Some N nodes work on reducing
        - Basically mapping and reducing work is split among nodes